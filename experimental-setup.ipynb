{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from torchvision.transforms import Compose, ToTensor, Grayscale, ToPILImage\n",
    "#import onnx\n",
    "#from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "#from onnx2pytorch import ConvertModel\n",
    "\n",
    "# Auxiliary Python imports\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "## Gym imports\n",
    "import gym\n",
    "import gym_jumping_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sarrg\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (60, 60)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym_jumping_task.envs.JumpTaskEnv()\n",
    "ACTION_SPACE = env.nb_actions\n",
    "floor_height = 10\n",
    "obstacle_pos = 30\n",
    "state = env._reset(floor_height=floor_height, obstacle_position=obstacle_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 60)\n"
     ]
    }
   ],
   "source": [
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2291abc8d60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFIUlEQVR4nO3dQWrEQAwAQU/ww/1z5dbHXGYWL6HqAbJ8agQGr5mZCwCu6/p5ewEAvocoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAuXcHrLVO7AHAITt/RHApABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMi9O2BmTuzxp7XWx58Bz/O8vcLH/Od34yyXAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAga2bm7SUA+A4uBQAiCgBEFACIKAAQUQAg9+6AtdaJPQA4ZOejUpcCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEDu3QEzc2IPAL6ASwGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACC//+QaBnXpJT4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(state, cmap='gray', origin='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Expert policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert():\n",
    "    def __init__(self, env: gym_jumping_task.envs.JumpTaskEnv):\n",
    "        self.env = env\n",
    "    \n",
    "    def select_action(self, state=None):\n",
    "        if self.env.agent_pos_x + 14 == self.env.obstacle_position:\n",
    "            return 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 57\n",
      "Game Won\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFIElEQVR4nO3dQYrDQBAEwRmjh+vn7Vuyd2mRMBEPGMqnpMGgPTOzAGCt9Xl6AADvIQoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAECOqw/sve/YAcBNrnwRwaUAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIMfTA+AtzvN8esK/+eXf9itm5ukJay2XAgB/iAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAkD0z8/QIAN7BpQBARAGAiAIAEQUAIgoA5Lj6wN77jh0A3OTKn0pdCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAOa4+MDN37ADgBVwKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIA+QKxiBoDs3hN2wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "expert = Expert(env)\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "    a = expert.select_action()\n",
    "    state, reward, done, _ = env.step(a)\n",
    "    score += reward\n",
    "print(\"Score:\", score)\n",
    "print(\"Game\", \"Won\" if reward == 2 else \"Lost\")\n",
    "plt.axis('off')\n",
    "plt.imshow(state, cmap='gray', origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and store demonstrations played by the expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transition = namedtuple('Transition', ['state', 'action', 'rewards', 'next_state', 'done'])\n",
    "Transition = namedtuple('Transition', ['state', 'action'])\n",
    "\n",
    "class Episode(object):\n",
    "    def __init__(self):\n",
    "        self.transitions = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.transitions)\n",
    "    \n",
    "    def append(self, transition):\n",
    "        self.transition.append(transition)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.transitions[index]\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity=0):\n",
    "        self.buffer = []\n",
    "        self._next = 0\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.buffer[index]\n",
    "    \n",
    "    def add(self, episode):\n",
    "        if self.capacity != 0 and len(self) == self.capacity:\n",
    "            self.buffer[self._next] = episode\n",
    "            self._next = (self._next+1)%self.capacity\n",
    "        else:\n",
    "            self.buffer.extend(episode)\n",
    "        \n",
    "    def save(self, path):\n",
    "        np.savez_compressed(path, np.asanyarray(self.buffer, dtype='object'))\n",
    "    \n",
    "    def load(self, path):\n",
    "        file = np.load(path, allow_pickle=True)\n",
    "        self.buffer = file.f.arr_0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72ded72cee74af3a4cc2a4741111745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_episodes = 250\n",
    "\n",
    "env = gym_jumping_task.envs.JumpTaskEnv()\n",
    "buffer = ReplayBuffer()\n",
    "for e in tqdm(range(n_episodes)):\n",
    "    episode = []\n",
    "    env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = expert.select_action()\n",
    "        episode.append(Transition(state, a))\n",
    "        state, reward, done, _ = env.step(a)\n",
    "    buffer.add(episode)\n",
    "#buffer.save(\"demonstrations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Demonstrations and create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 56000\n"
     ]
    }
   ],
   "source": [
    "buffer = ReplayBuffer()\n",
    "buffer.load(\"demonstrations.npz\")\n",
    "print(\"Episodes:\", len(buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferDataset(Dataset):\n",
    "    def __init__(self, buffer):\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.buffer[index]\n",
    "        state = sample.state\n",
    "        _action = sample.action\n",
    "        action = torch.zeros(ACTION_SPACE)\n",
    "        action[_action] = 1.0\n",
    "        return torch.tensor(state).to(device).unsqueeze(dim=0), torch.tensor(action).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]), tensor([1., 0.]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarrg\\AppData\\Local\\Temp\\ipykernel_18372\\1573719655.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(state).to(device).unsqueeze(dim=0), torch.tensor(action).to(device)\n"
     ]
    }
   ],
   "source": [
    "dataset = BufferDataset(buffer)\n",
    "print(dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent and Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentNetwork(nn.Module):\n",
    "    def __init__(self, n_action_space):\n",
    "        super(AgentNetwork, self).__init__()\n",
    "        \n",
    "        # based on LeNet\n",
    "        self.c1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.s2 = torch.nn.MaxPool2d(4)\n",
    "        self.c3 = torch.nn.Conv2d(6, 16, 5)\n",
    "        self.s4 = torch.nn.MaxPool2d(2)\n",
    "        self.c5 = torch.nn.Conv2d(16, 120, 5)\n",
    "        self.f6 = torch.nn.Linear(120, n_action_space*5)\n",
    "        self.d7 = torch.nn.Dropout(0.5)\n",
    "        self.out = torch.nn.Linear(n_action_space*5, n_action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.c1(x))\n",
    "        x = self.s2(x)\n",
    "        x = torch.relu(self.c3(x))\n",
    "        x = self.s4(x)\n",
    "        x = torch.relu(self.c5(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.f6(x))\n",
    "        x = self.d7(x)\n",
    "        return self.out(x)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform(state):\n",
    "        return torch.tensor(state).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def act(self, state):\n",
    "        self.model.eval()\n",
    "        state = self.transform(state)\n",
    "        actions = self.model(state).squeeze()\n",
    "        return torch.argmax(actions).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCTrainer(object):\n",
    "    \"\"\"Behavioral Cloning Trainer class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, dataloader, loss_func, optimizer):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def train(self, pbar=None):\n",
    "        self.model.train()\n",
    "        if pbar is None:\n",
    "            pbar = tqdm(range(1))\n",
    "        \n",
    "        loader_len = len(self.dataloader)\n",
    "        for i, (states, actions) in enumerate(self.dataloader):\n",
    "            pred = self.model(states)\n",
    "            loss = self.loss_func(pred, actions)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            pbar.set_postfix_str(f\"[{i+1}/{loader_len}] Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "            \n",
    "    def run(self, epochs):\n",
    "        \n",
    "        with tqdm(range(epochs)) as pbar:\n",
    "            for e in pbar:\n",
    "                loss = self.train(pbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batchsize = 250\n",
    "n_epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)\n",
    "torch.manual_seed(8)\n",
    "\n",
    "net = AgentNetwork(ACTION_SPACE).to(device)\n",
    "loss_func = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "trainer = BCTrainer(net, loader, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2259bb2607034c1b96c58a2f2ca4267d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarrg\\AppData\\Local\\Temp\\ipykernel_18372\\1573719655.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(state).to(device).unsqueeze(dim=0), torch.tensor(action).to(device)\n"
     ]
    }
   ],
   "source": [
    "trainer.run(n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Behavioral Cloning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(agent, show_progress=True, record_video=True, show_hud=False):\n",
    "    env = Env(img_stack=1, record_video=record_video, show_hud=show_hud)\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    done_or_die = False\n",
    "    if show_progress:\n",
    "        progress = tqdm(desc=\"Score: 0\")\n",
    "    while not done_or_die:\n",
    "        action, action_idx, a_logp = agent.select_action(state)\n",
    "        state, reward, done, die = env.step(action)\n",
    "        score += reward\n",
    "        if show_progress:\n",
    "            progress.update()\n",
    "            progress.set_description(\"Score: {:.2f}\".format(score))\n",
    "        if done or die:\n",
    "            done_or_die = True\n",
    "    env.close()\n",
    "    if show_progress:\n",
    "        progress.close()    \n",
    "    if record_video:\n",
    "        show_video()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3242121a426c4a3b9076c62c44b8a0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_episodes = 50\n",
    "env = gym_jumping_task.envs.JumpTaskEnv(rendering=True)#, slow_motion=True)\n",
    "agent = Agent(net)\n",
    "won = 0\n",
    "with tqdm(range(n_episodes)) as pbar:\n",
    "    for e in pbar:\n",
    "        env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = agent.act(state)\n",
    "            state, reward, done, _ = env.step(a)\n",
    "\n",
    "        if reward == 2:\n",
    "            won += 1\n",
    "        pbar.set_postfix_str(f\"Won: {won}/{n_episodes}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46d7f8e2fb6de0d48818e7e4142342fa23f3a8704131dfe3fc9fa223bc303aad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
